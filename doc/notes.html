<h1 id="coordinate-considerations">Coordinate Considerations</h1>
<p>Working with the OpenGL ecosystem invariably requires an understanding of the different coordinate systems used for the polygon vertices, textures and screen. This becomes even more important when using OpenGL for computation, as being off by one pixel (or a fraction of a pixel) can have much more serious consequences than mere graphical glitches. If a texture is used as a lookup table for arbitrary information, it is essential that the correct values are indexed, to avoid giving, at best, completely incorrect results, or at worst hard-to-detect bugs due to the limitations of floating point precision in the 0 to 1 range used to index textures.</p>
<p>Firstly we have the window (or screen) coordinates, which give the position in the viewport, in other words the final image output. However, since the output may be rendered to a texture, window coordinates don't have to be related to an image actually displayed on the screen. They are similar to pixel positions, however OpenGL itself does not have a concept of a pixel until rasterisation. <span class="citation">Peers (2002)</span> gives a detailed mathematical treatment of OpenGL coordinates, drawing from the OpenGL specification. In this way, the viewport can be treated as a Cartesian plane whose origin and unit vectors are given by the <code>gl.viewport(x,y,w,h)</code> command. This sets the x,y offset of the origin, which is at the bottom-left edge of the image, and determines the area of the scene which should be rasterised, so in a graphics sense can be considered a sort of cropping of the image. Two important points to note here are that the Y axis is effectively flipped relative to the coordinate system usually used in graphics, which has the origin at the top-left, and that integer coordinates will index the bottom left corners of pixels, so to index the centre of a pixel requires adding 0.5 to each dimension. For general purpose computation on a grid, modifying the viewport can be used to change the output range of the computation. For example, when doing face detection at different scales, the &quot;sliding window&quot; of the detector will change size, meaning less pixel positions need to be considered for larger windows, so the size of the output grid should be smaller.</p>
<div class="figure">
<img src="./facescale.png" alt="Decreased output range for larger window" /><p class="caption">Decreased output range for larger window</p>
</div>
<p>The vertex positions of polygons are specified by setting the <code>gl_Position</code> variable in the vertex shader. This is a four dimensional <code>(x,y,z,w)</code> vector where x,y,z are in Normalised Device Coordinates, a resolution-independent coordinate system which varies from -1 to 1 in each dimension such that the origin is at the centre. These then undergo perspective division by the fourth <code>gl_Position.w</code> coordinate. For convenience we can use window coordinates when we supply the vertices as an attribute, then compute the normalised coordinates in the vertex shader by dividing by the image resolution. This will give a value in the range [0,1], which can be changed to the range [-1,1] by multiplying by 2 then subtracting 1. For the purposes of computation on a 2D grid, the only geometry we need is a rectangle aligned with the viewport, which we can get by drawing two triangles. We do not want any perspective division, so z,w can be set to 0,1. This effectively &quot;passes through&quot; the vertex coordinates, allowing us to use them as if they were window coordinates.</p>
<p>The shader code to achieve this is: (where aPosition is the vertex position attribute and uResolution gives the image resolution)</p>
<pre class="sourceCode C"><code class="sourceCode c">vec2 normCoords = ((aPosition/uResolution) * <span class="fl">2.0</span>) - <span class="fl">1.0</span>;
gl_Position = vec4(normCoords, <span class="dv">0</span>, <span class="dv">1</span>);</code></pre>
<p>Finally, we have to deal with the coordinates of texture maps, made up of texels (the texture equivalent of a pixel) which are sampled using texture2D() in the fragment shader. They have coordinates from 0,0 in the bottom left to 1,1 in the top right. Textures may be sampled using different filtering methods in order to interpolate between the discrete texels, the simplest being &quot;NEAREST&quot; which simply uses the closest texel value, and &quot;LINEAR&quot; which interpolates linearly based on the distance to surrounding texels. To sample at precisely the texel centre, with no filtering, it is necessary to offset by half a texel, since the &quot;zero&quot; of a texel is at the bottom left corner. So for the ith texel in a row we would use X coordinate <code>(i + 0.5)/width</code> to offset then normalise to the [0,1) range.</p>
<h1 id="initial-implementation">Initial Implementation</h1>
<p>The main strategy for the initial implementation of face detection in WebGL is to offload the lookup operations on the integral image, the calculation of the Local Binary Pattern values, and the subsequent window evaluation to the fragment shader. This allows the &quot;sliding window&quot; to be parallelised so that we are evaluating multiple window positions at once. For each stage in the face detection cascade we have to compute the Local Binary Pattern for various rectangles within the window. Depending on which pattern we get for a rectangle, it may either contribute a positive or a negative weighting towards the window being a face. Summing the weights contributed by all the rectangles and comparing against an overall threshold for the stage, we determine whether the window should be rejected outright or subjected to further scrutiny in later stages. Computing each Local Binary Pattern rectangle requires 16 texture lookups, since we have to subdivide the rectangle into 3x3, giving 9 blocks, and compare the intensity of the centre block with the 8 surrounding blocks. Using the integral image technique, finding the intensity of a block of any area requires just four texture lookups, so for each rectangle we need to sample 4x9 = 16 points.</p>
<div class="figure">
<img src="./lbpwindow.png" alt="Window for a stage with 3 rectangles" /><p class="caption">Window for a stage with 3 rectangles</p>
</div>
<p>The data on which Local Binary Patterns should be considered positive or negative is accessed from the shader by using a grayscale texture as a lookup table. There is one row for each stage in the cascade, so the height is the number of stages, and for each LBP rectangle we have 256 possible patterns. A black pixel is used to indicate a positive pattern, a white pixel a negative pattern. The width of the texture is then 256 x the maximum number of rectangles. For the default cascade used, we have 20 stages and a maximum of 10 LBP rectangles, giving a <span class="math">10 × 2560</span> texture. This differs from the more compact representation used by OpenCV, which packs the data for one rectangle into 256 bits (8 32-bit ints) per rectangle but is necessary because the GL shader language does not support the bitwise operations needed to extract the individual bits (since numbers may in fact be implemented as floating point in hardware), nor the range needed for 32-bit integers.</p>
<p><img src="image-1.png" /> </p>
<p>A shader program is compiled for each stage of the cascade, based on the same shader source code but using compiler #defines to modify certain constants, such as the number of rectangles. This is because loop conditions in GLSL must be based on constant expressions, so the number of rectangles to loop over cannot be passed as variable. Each stage writes out a texture with a white pixel for each window accepted, a black pixel otherwise. The texture from the previous stage is used as an input to the next stage, to avoid computing windows which have already been rejected.</p>
<p>On top of this loop over stages, we also need to consider different scales, to be able to detect faces of different sizes in the image. This is done by setting a scale factor, such as 1.2, which we successively multiply the window size and rectangle offsets by. We run the detection for each scale, starting from the base 24x24 pixel window size, until some maximum where the window would be too big to fit in the image.</p>
<p>After detection is run on each scale, the accepted window texture is read back to a JavaScript array using the WebGL <code>gl.readPixels</code> command, and used to draw appropriately sized rectangles at the locations where faces have been found.</p>
<div class="figure">
<img src="./detectscales.png" alt="Output of detection at different scales" /><p class="caption">Output of detection at different scales</p>
</div>
<h1 id="achieving-higher-performance">Achieving Higher Performance</h1>
<p>In order to test how fast this initial implementation is we can insert some timer calls. We measure the time for each scale as well as the overall time for the detection call (after the inital setup of shaders and textures), on an image of dimensions 320x240 containing three faces of different scales. This gives the output shown below.</p>
<div class="figure">
<img src="./initialtiming.png" alt="Initial timing" /><p class="caption">Initial timing</p>
</div>
<p>This gives an overall time of 375 milliseconds, obviously not good enough for real time detection. Looking more closely, the majority of time seems to be spent on the first scale, which takes 212 ms, whereas the other scales take 10ms or less. Using the Chrome Javascript Profiler tool we can investigate further by checking which functions are taking up the most time.</p>
<div class="figure">
<img src="./initialprofile.png" alt="Javascript Profile" /><p class="caption">Javascript Profile</p>
</div>
<p>This shows that (besides the initial overhead of setting up the shaders) most of the time is spent in the <code>gl.readPixels</code> function, responsible for transferring image data from GPU memory back to JavaScript. An easy way to see just how responsible this function is for the slowdown is to simply comment the <code>readPixels</code> calls and associated code for drawing rectangles, which gives the following timings:</p>
<div class="figure">
<img src="./timingnoreadpixels.png" alt="Timing without readPixels" /><p class="caption">Timing without readPixels</p>
</div>
<p>This shows a massive improvement, bringing the time down to 8ms, but obviously our face detection is not very useful if we cannot actually get the locations of the faces at the end!</p>
<p>The previous results were timed using a single image, running the detection once after the page loads. In a real scenario we would want to be detecting continually on each frame. This leads us to investigate the result of running the detection on two different images, one after the other, without refreshing the page. (In fact the same image, but flipped horizontally, so we would expect similar face detection results, but avoid any clever caching by the browser).</p>
<div class="figure">
<img src="./timingtwoimages.png" alt="Timing on two images" /><p class="caption">Timing on two images</p>
</div>
<p>This gives the surprising result that, while the first run of the detection takes a long time, the second is considerably shorter, with times between 2 and 10 ms for each scale. While we cannot determine the exact cause of this, it seems that from a &quot;cold start&quot;, readPixels has some overhead which is not experienced on subsequent calls. So while readPixels is still the slowest factor, once the detection gets going we need not worry about reads taking over 100ms. From here, the best strategy to improve overall time seems to be to minimise the number of readPixels calls needed, ideally with just one at the end of detection rather than intermediate calls for each scale.</p>
<p>While refactoring the code to &quot;pingpong&quot; by flipping between multiple framebuffers, rather than the more expensive technique of using one framebuffer and attaching different textures in turn (as recommended by <span class="citation">Tavares (2011)</span> at 37m20s), it was discovered that the slowdown on the first readPixels seemed to disappear. However, after some work to narrow down the exact conditions which would produce the slowdown, it was determined that this optimisation alone was not responsible for the difference, but rather that it was determined by the ordering of the calls to attach textures to the framebuffers, relative to the code setting up the shaders. It turned out that, if at least one <code>gl.framebufferTexture2D</code> call was before the shader setup, the initial readPixels call took 10ms, whereas otherwise it took over 200ms. The initial setup which includes compiling the shaders always takes around half a second, so while the order of calls does not change the initial setup time, it allows the &quot;warm up&quot; time required before readPixels to effectively be hidden behind the time needed to compile the shaders. This is likely because the shader compilation is mostly CPU-bound, allowing other tasks to be done in parallel on the GPU.</p>
<p>In order to eliminate the intermediate readPixel calls, we need to write the output from each scale to the same texture, preserving the pixels output from the previous scale, and encoding the scale in the pixel value. To indicate the scale of an accepted window we can simply write out the ordinal number (1,2,3...) of the scale as a colour value, or 0 if the window is rejected. The size to multiply the rectangle by is then <span class="math"><em>s</em><em>c</em><em>a</em><em>l</em><em>e</em><em>F</em><em>a</em><em>c</em><em>t</em><em>o</em><em>r</em><sup>(<em>s</em><em>c</em><em>a</em><em>l</em><em>e</em><em>N</em><em>u</em><em>m</em><em>b</em><em>e</em><em>r</em> − 1)</sup></span>. The use of two textures to &quot;pingpong&quot; the results between each stage in a scale remains as before, except that on the final stage we write to a shared final output texture. One limitation is that, if we have two detections of different scales at exactly the same position, the later (larger) scale will overwrite the previous one. However, this should be a relatively rare occurence, and should not make too much difference when all the rectangles are grouped to find the final face positions. Another complication is that we want to keep the previous written pixels, instead of writing a black pixel for a rejected window in a subsequent scale. The simplest way to prevent output of any pixel at all is to use the <code>discard;</code> statement in the fragment shader. However, in certain cases (discussed in <span class="citation">Jave (2011)</span>) this may invoke a performance penalty, particularly on mobile GPUs. An alternative is to use OpenGL's blend modes, which specify how pixels written should be blended with the pixels already present.</p>
<div class="figure">
<img src="./scalessametexture.png" alt="Writing out pixels for each scale. A lighter colour pixel indicates a larger detection" /><p class="caption">Writing out pixels for each scale. A lighter colour pixel indicates a larger detection</p>
</div>
<p>First an implementation was created using <code>discard;</code>, giving an average time of 71ms per detection run (for a 320x240 image over 20 runs), compared to 110ms using <code>readPixels</code> for each scale under equivalent conditions.</p>
<p>The implementation was then adapted to use blending, in order to test which would give the best performance. As explained in <span class="citation">Thomas (2009)</span>, the <code>gl.blendFunc(sfactor, dfactor)</code> function sets the factors which the source (being drawn) and destination (already in the framebuffer) should be multiplied by, where <code>sfactor</code> and <code>dfactor</code> are symbolic constants determining where the factors should come from. The output for each colour channel is given by <span class="math"><em>R</em><em>e</em><em>s</em><em>u</em><em>l</em><em>t</em> = <em>S</em><em>o</em><em>u</em><em>r</em><em>c</em><em>e</em><em>V</em><em>a</em><em>l</em> × <em>S</em><em>o</em><em>u</em><em>r</em><em>c</em><em>e</em><em>F</em><em>a</em><em>c</em><em>t</em><em>o</em><em>r</em> + <em>D</em><em>e</em><em>s</em><em>t</em><em>V</em><em>a</em><em>l</em> × <em>D</em><em>e</em><em>s</em><em>t</em><em>F</em><em>a</em><em>c</em><em>t</em><em>o</em><em>r</em></span>. We set <code>sfactor</code> to <code>SRC_ALPHA</code> and <code>dfactor</code> to <code>ONE_MINUS_SRC_ALPHA</code>, which means that when outputting <code>gl_FragColor</code> we can set the alpha value to 0.0 to completely preserve the existing pixel.</p>
<p>Comparing the timing of the two techniques over 100 iterations, there turned out to be almost no difference in the mean time, at least on a laptop Intel GPU, although as shown in the box plot the Blend version had a slightly greater variance. In the end the Blend version was preferred, to avoid potential slowness with other GPUs and because it allowed the shader code to be simplified, eliminating a branching condition to explicitly check if the window was rejected.</p>
<div class="figure">
<img src="./blend-vs-discard.png" alt="Box plot of timings using Discard vs Blend, for 100 iterations" /><p class="caption">Box plot of timings using Discard vs Blend, for 100 iterations</p>
</div>
<!--commit cce857eb3ef56be9aaf6d2df1e1cfe6665c74b9a -->

<h2 id="timing-stages">Timing stages</h2>
<p>In order to analyse the times of operations at a finer granularity, we want to time each draw call individually. However, because the CPU and GPU operate asynchronously, each draw call will in fact return immediately, and the CPU will only wait for the GPU to finish when some operation requiring information from a framebuffer is performed. Therefore, we insert a dummy readPixels operation, reading only 1 pixel, after each draw. Because some times are very small (below 1ms) and difficult to measure accurately, we also artificially repeat each draw operation 10 times, and divide the total time by 10. In this way, we can obtain a detailed profile of how much time is spent running the shader for each stage and scale.</p>
<p>We observe that, as expected more time is spent in the early stages, because the first stage must run on all windows, whereas for laters stages some windows are rejected. Increasing the scale also shows a decrease in time, since less window positions need to be evaluated, although this is only really noticeable in the first two stages, the subsequent stages showing around the same time regardless of scale.</p>
<div class="figure">
<img src="./stagetimes.png" alt="Times of stages and scales" /><p class="caption">Times of stages and scales</p>
</div>
<div class="figure">
<img src="./scaletime.png" alt="Times for the first 3 stages at different scales" /><p class="caption">Times for the first 3 stages at different scales</p>
</div>
<p>What is interesting to note is that the first three stages take up 48% of the time, while the remaining 17 stages take up 52% of the time. So while it is tempting to try to chip away at the above-2ms times in the early stages, we have a &quot;long tail&quot; effect where the sub-0.5ms times of later stages add up to a significant proportion of the overall time. Therefore, treating the early stages as special cases (such as manually fine-tuning the shader code for these specific stages) is unlikely to provide much of an advantage, compared to general techniques that apply equally to the later stages.</p>
<h2 id="why-are-the-shaders-slow">Why are the shaders slow?</h2>
<p>At an abstract level, all the fragment shaders are doing is</p>
<ol style="list-style-type: decimal">
<li>Looking up some values in the integral image and LBP lookup textures</li>
<li>Doing some maths to determine what value to output</li>
</ol>
<p>Now, GPUs are typically very fast at carrying out floating point calculations, so we wouldn't expect the &quot;maths&quot; portion to be overly challenging. <span class="citation">Harris (2005)</span> explains this using the concept of &quot;arithmetic intensity&quot;, the ratio of computation to bandwidth.</p>
<pre><code>arithmeticIntensity = operations/wordsTransferred</code></pre>
<p>According to Harris, applications that benefit most from GPU acceleration are those with high arithmetic intensity, where &quot;The data communication required to compute each element of the output is small and coherent&quot;. So ideally, the amount of data fetched from textures would be small, and would be spatially localised, in order to take advantage of caching. Unfortunately, in order to calculate the 9 blocks of the rectangle for each classifier, we require 16 texture lookups, and the positions fetched for a window are not guaranteed to be close together. Since the number of weak classifier rectangles can vary from 3 in the first stage to 10 in the later stages, we are talking about <span class="math">3 × 16 = 48</span> at best and <span class="math">10 × 16 = 160</span> at worst texture lookups. For the base scale they will at least be within the same <span class="math">24 × 24</span> area, but when the window is scaled we will be fetching values locations more spread out over the image. Texture caches are typically optimised for some 2D neighbourhood of a few texels, which great for applications such as convolution where we just need to look up adjacent texels, but is not ideal for more general purpose approaches.</p>
<p>To test the theory that the texture fetches are responsible for most of the slowdown, we create a test shader which performs the same texture fetches as our face detection shader but does not do anything useful with the result (instead just outputting the sum of the values, to ensure the fetches are not optimised out). Performing the same texture fetches as the 1st stage of the cascade (48 fetches), and timing over 1000 iterations, we get an average time of 3.1 ms per draw call, which is pretty much identical to the full shader. Further, commenting out half the fetches reduces the time to 1.3ms, clearly showing the impact of texture fetching on the time.</p>
<p>TODO: Things tried that made no difference:</p>
<ul>
<li>Moving code to calculate rectangle offsets into vertex shader</li>
<li>Using UNSIGNED_BYTE texture (is faster if just reading one component (byte), but once we access all it is just as slow as FLOAT texture)</li>
<li>Iterating over scales within the shader (just made the &quot;multiscale&quot; shader around as slow as the combined time for different scales, and makes it difficult to track which windows accepted, since we need to encode for each scale somehow)</li>
</ul>
<p>Optimisations to look at:</p>
<ul>
<li>&quot;Stage-parallel&quot; processing - compute weak classifiers over a larger window at once, as in <span class="citation">Obukhov (2011)</span> - problem: Can only output four bytes for each fragment</li>
<li>Split some work between CPU and GPU. Since the different scales can be computed independently, could hand off some portion of the scales to CPU to be processed simultaneously (but would then lose ability to use CPU for other tasks)</li>
<li>Reduce number of texture accesses by &quot;factorisation&quot; of the LBP pattern - eg if we know the top left block should never be zero, can return negative from classifier after just computing centre and top left blocks. Problems: branching, and how to represent the &quot;factoring&quot; data in the shader (if it requires fetching more values from texture could do more harm than good!)</li>
<li>z-Culling: use the depth buffer to indicate rejected windows, so that the fragment shader doesn't run at all for these pixels (in theory should offer some speedup by not running fragment shader at all on blocks of some size, and will avoid having to read the &quot;activeWindows&quot; texture, but doing the depth testing may have some penalty, and maybe the compiler is already clever enough)</li>
</ul>
<div class="figure">
<img src="./varyings-nodifference.png" alt="Calculating offsets for texture lookup with varyings in the vertex shader (green) vs fragment shader (red) gives no difference in timing, around 3.5ms in each case. The output values are simply the sum of all texture values mod 255, giving not very meaningful output, but showing that identical values are computed" /><p class="caption">Calculating offsets for texture lookup with varyings in the vertex shader (green) vs fragment shader (red) gives no difference in timing, around 3.5ms in each case. The output values are simply the sum of all texture values mod 255, giving not very meaningful output, but showing that identical values are computed</p>
</div>
<h1 id="optimisation-techniques">Optimisation Techniques</h1>
<h2 id="the-perils-of-branching">The Perils of Branching</h2>
<p>Branching within the shader, while possible through the use of if-else statements, carries with it numerous caveats, explained in <span class="citation">Harris &amp; Buck (2005)</span>. In the &quot;olden days&quot; (say, 2003) in order to emulate branching, GPUs would simply evaluate both sides of the condition, then determine which result to use before writing the output. This meant that the time would be proportional to the cumulative cost of both branches.</p>
<p>Things got better with the SIMD (Single Instruction, Multiple Data) model, which uses multiprocessors executing the same instruction on many data elements at once. In this case, the GPU will not be doing useless work evaluating both sides of the condition, but instead divergent branches will cause a stall, where the processors that do not take a branch have to wait for the branching processors to catch up. In the worst case this will still take as long as both branches combined, but in the case where all processors take the same branch (known as coherency) it will be more efficient, and since the allocation of fragments to processors is often done in a spacially localised manner, it allows for speedups when fragments in the same area of an image branch in the same way.</p>
<p>Finally, true dynamic branching may be available in the form of MIMD (Multiple Instructions, Multiple Data) where different processors may execute different instructions simultaneously. Most modern GPUs support dynamic branching to some extent (NVIDIA's GeForce 6 series, released in 2005, introduced MIMD branching in the fragment shader) however at an architectural level branching still presents a barrier to efficient parallel computation, since knowing that all fragments will follow the same instructions gives the GPU opportunities for optimisation.</p>
<p>For this reason, branching in the shader should be kept to a minimum, and it is preferred for algorithms to be structured such that fragments in the same neighbourhood take the same branches in order to maximise coherency. Especially in the case of WebGL, the programmer has no control over what graphics card capability the user will have, and is unable to query information about the graphics card due to security restrictions, so it is best to program for the lowest common denominator.</p>
<p>TODO: Z-Culling</p>
<h1 id="bibliography">Bibliography</h1>
<p>Harris, M. (2005) Mapping Computational Concepts to GPUs. In: Matt Pharr (ed. &amp; tran.). <em>GPU Gems 2</em>. Addison Wesley. p. 493.</p>
<p>Harris, M. &amp; Buck, I. (2005) GPU Flow-Control Idioms. In: Matt Pharr (ed. &amp; tran.). <em>GPU Gems 2</em>. Addison Wesley. p. 547.</p>
<p>Jave (2011) <em>Is discard bad for program performance in OpenGL? - Stack Overflow</em>. [Online]. Available from: <a href="http://stackoverflow.com/questions/8509051/is-discard-bad-for-program-performance-in-opengl" title="http://stackoverflow.com/questions/8509051/is-discard-bad-for-program-performance-in-opengl">http://stackoverflow.com/questions/8509051/is-discard-bad-for-program-performance-in-opengl</a> [Accessed: 27AD–May 13AD].</p>
<p>Obukhov, A. (2011) Haar Classifiers for Object Detection with CUDA. In: Wen-Mei Hwu (ed. &amp; tran.). <em>GPU Computing Gems</em>. Morgan Kaufmann. p. 517.</p>
<p>Peers, B. (2002) <em>OpenGL pixel and texel placement</em>. [Online] Available from: <a href="http://bpeers.com/articles/glpixel/" title="http://bpeers.com/articles/glpixel/">http://bpeers.com/articles/glpixel/</a>.</p>
<p>Tavares, G. (2011) <em>Google I/O 2011: WebGL Techniques and Performance</em>. [Online]. Available from: <a href="http://www.youtube.com/watch?v=rfQ8rKGTVlg" title="http://www.youtube.com/watch?v=rfQ8rKGTVlg">http://www.youtube.com/watch?v=rfQ8rKGTVlg</a> [Accessed: 27 May 2013].</p>
<p>Thomas, G. (2009) <em>WebGL Lesson 8 – the depth buffer, transparency and blending</em>. [Online]. Available from: <a href="http://learningwebgl.com/blog/?p=859" title="http://learningwebgl.com/blog/?p=859">http://learningwebgl.com/blog/?p=859</a> [Accessed: 27AD–May 13AD].</p>
